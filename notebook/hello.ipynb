{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NUM_SAMPLES = 1\n",
    "MAX_NEW_TOKENS= 50\n",
    "TOP_K = 200\n",
    "TEMPERATURE = 0.8\n",
    "ACCELERATOR = \"AUTO\"\n",
    "MODEL_SIZE = \"7B\"\n",
    "QUANTIZE = True\n",
    "\n",
    "ROOT_DIR = \"/app\"\n",
    "CHECKPOINT_PATH = Path(f\"{ROOT_DIR}/lit-llama/checkpoints/lit-llama/{MODEL_SIZE}/state_dict.pth\")\n",
    "TOKENIZER_PATH = Path(\"{ROOT_DIR}/lit-llama/checkpoints/lit-llama/tokenizer.model\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from lit_llama import LLaMA, Tokenizer, as_8_bit_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: torch.nn.Module,\n",
    "    idx: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    max_seq_length: int,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        idx: Tensor of shape (B, T) with indices of the prompt sequence.\n",
    "        max_new_tokens: The number of new tokens to generate.\n",
    "        max_seq_length: The maximum sequence length allowed.\n",
    "        temperature: Scales the predicted logits by 1 / temperature\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities\n",
    "    \"\"\"\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    B, T = idx.shape\n",
    "    T_new = T + max_new_tokens\n",
    "    empty = torch.empty(B, T_new, dtype=idx.dtype, device=idx.device)\n",
    "    empty[:, :T] = idx\n",
    "    idx = empty\n",
    "\n",
    "    # generate max_new_tokens tokens\n",
    "    for t in range(T, T_new):\n",
    "        # ignore the not-filled-yet tokens\n",
    "        idx_cond = idx[:, :t]\n",
    "        # if the sequence context is growing too long we must crop it at max_seq_length\n",
    "        idx_cond = idx_cond if T <= max_seq_length else idx_cond[:, -max_seq_length:]\n",
    "\n",
    "        # forward\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # concatenate the new column\n",
    "        idx[:, t:] = idx_next\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
